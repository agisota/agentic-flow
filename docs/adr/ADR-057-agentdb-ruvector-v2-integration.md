# ADR-057: AgentDB/RuVector Deep Integration for Agentic-Flow V2

## Status

Accepted

## Date

2026-02-24

## Context

A deep review of the agentic-flow codebase reveals that AgentDB v2 provides 21 production-ready controllers and 8 RuVector packages, but agentic-flow uses approximately **30% of available capabilities**. The remaining 70% — including semantic routing, trajectory learning, attention-based coordination, hypergraph state, and causal reasoning — could transform agentic-flow from a stateless agent orchestrator into a self-improving swarm with persistent memory, learned routing, and explainable decisions.

### Current Integration State

**What agentic-flow uses:**
- `RuVectorBackend` — HNSW vector storage (working)
- `RuVectorLearning` — Optional GNN enhancement (not actively trained)
- `AttentionService` — 5 mechanisms available, JS fallback only
- `ReasoningBank` — VectorBackend search, optional GNN
- `GraphDatabaseAdapter` — Initialized but never called

**What agentic-flow does NOT use (18+ controllers):**

| Controller | Capability | Impact If Integrated |
|-----------|-----------|---------------------|
| ReflexionMemory | Episodic replay with self-critique | Agents learn from mistakes |
| SkillLibrary | Lifelong skill management & composition | Cross-agent skill reuse |
| LearningSystem | 9 RL algorithms (DQN, PPO, A3C, MCTS, etc.) | Adaptive agent behavior |
| CausalMemoryGraph | Do-calculus, causal uplift, interventions | Evidence-based routing |
| CausalRecall | Utility ranking: U = α*sim + β*uplift − γ*latency | Causal-aware memory |
| ExplainableRecall | Merkle proof provenance, policy certificates | Auditable decisions |
| NightlyLearner | Automated batch learning, A/B experiments | Self-improving swarm |
| SyncCoordinator | Bidirectional sync, conflict resolution | Multi-instance consistency |
| QUICServer/Client | Low-latency agent-to-agent sync | Sub-second state sharing |
| EmbeddingService | Sentence-transformers with caching | Semantic understanding |
| EnhancedEmbeddingService | Multi-model late fusion | Higher quality embeddings |
| MMRDiversityRanker | Maximum Marginal Relevance ranking | Diverse search results |
| ContextSynthesizer | Pattern-aware context synthesis | Richer agent context |
| MetadataFilter | Rich AND/OR/NOT filtering | Precise memory queries |

### Unused RuVector Packages

| Package | Version | Capability | Status |
|---------|---------|-----------|--------|
| `@ruvector/router` | 0.1.15 | Semantic intent routing with learned preferences | Never called |
| `@ruvector/sona` | 0.1.5 | Trajectory learning, micro-LoRA, EWC++ forgetting prevention | Not integrated |
| `@ruvector/graph-node` | 0.1.15 | Hypergraph DB, Cypher queries, temporal hyperedges | Installed, not used |
| `@ruvector/rvf` | — | 10-100x storage compression, eBPF optimization | Not used |
| `@ruvector/attention` | 0.1.31 | 5 NAPI attention mechanisms | JS fallback only |

### Current Agentic-Flow Memory Architecture (Gaps)

All agent coordination in agentic-flow uses **in-memory Maps** that are lost on restart:

| System | Current Storage | Current Matching | Problem |
|--------|----------------|-----------------|---------|
| Agent Memory | `Map<string, any>` | None — ephemeral | All state lost on restart |
| Pattern Storage | `Map<string, Pattern>` | Jaccard similarity (0.5 threshold) | No vector embeddings |
| Routing | Hardcoded keyword synonyms | String matching | Cannot learn from outcomes |
| Confidence Scoring | Static weights | Fixed formula | No domain adaptation |
| Escalation | Sequential rules | Hard-coded thresholds | No feedback loop |
| Knowledge Search | `Map<string, any>` | Linear O(n) scan | No semantic search |

### Key Source Locations

**Agentic-flow integration stubs:**
- `src/mcp/agentdb-integration.ts` — Commented-out imports, in-memory Maps
- `src/services/agentdb-learning.service.ts` — Placeholder implementations
- `src/middleware/agentdb-integration.ts` — Basic pattern storage (Map)
- `src/verification/learning/agentdb-integration.ts` — Standalone learning

**Agentic-flow routing (keyword-based):**
- `src/routing/provider-matcher.ts` — Hardcoded synonym maps
- `src/routing/severity-classifier.ts` — Static keyword lists
- `src/routing/escalation-router.ts` — Sequential rules, no learning
- `src/routing/emergency-detector.ts` — Manual weights

**MCP tools (no agentdb):**
- `agentic-flow/src/mcp/fastmcp/tools/swarm/orchestrate.ts` — CLI wrapper
- `agentic-flow/src/mcp/fastmcp/tools/swarm/spawn.ts` — Simple invocation

## Decision

### Architecture: AgentDB as Agentic-Flow's Intelligence Layer

```
Agentic-Flow V2 + AgentDB Integration
├── Agent Coordination Layer (existing)
│   ├── Swarm Init / Spawn / Orchestrate
│   └── MCP Tool Dispatch
│
├── NEW: Persistent Memory Layer (AgentDB)
│   ├── ReflexionMemory — Episode storage & replay
│   ├── SkillLibrary — Cross-agent skill sharing
│   ├── ReasoningBank — Pattern storage & retrieval
│   └── RuVectorBackend — <100µs HNSW search
│
├── NEW: Learned Routing Layer (RuVector)
│   ├── @ruvector/router — Semantic intent routing
│   ├── @ruvector/sona — Trajectory learning + EWC++
│   ├── CausalMemoryGraph — Causal routing policies
│   └── CausalRecall — Utility-based task assignment
│
├── NEW: Attention-Based Coordination (RuVector)
│   ├── HyperbolicAttention — Hierarchical agent teams
│   ├── FlashAttention — Large-scale agent comms
│   ├── MoEAttention — Specialized agent dispatch
│   └── MultiHeadAttention — Multi-factor decisions
│
├── NEW: Graph State Layer (RuVector)
│   ├── @ruvector/graph-node — Hypergraph state
│   ├── GraphDatabaseAdapter — Cypher queries
│   ├── Temporal hyperedges — Time-aware tracking
│   └── SyncCoordinator — Multi-instance sync
│
└── NEW: Self-Improvement Layer (AgentDB)
    ├── NightlyLearner — Automated batch learning
    ├── LearningSystem — 9 RL algorithms
    ├── ExplainableRecall — Merkle proof decisions
    └── @ruvector/sona — Zero-cost background training
```

### Phase 1: Persistent Agent Memory (Week 1-2)

**Goal**: Replace in-memory Maps with AgentDB-backed persistent storage.

**1.1 Episode Memory via ReflexionMemory**
- Store every agent task execution as an episode (input, actions, outcome, reward)
- Enable experience replay: before starting a task, query similar past episodes
- Self-critique: agents evaluate their own past performance

**Implementation**:
```typescript
// In agent task execution path
const reflexion = new ReflexionMemory(agentdb);
await reflexion.storeEpisode({
  taskType, agentType, actions, outcome, reward, timestamp
});

// Before new task
const similar = await reflexion.findSimilar(newTask, { limit: 5 });
const context = similar.map(ep => ep.selfCritique).join('\n');
```

**Files to modify**:
- `src/mcp/agentdb-integration.ts` — Replace Maps with ReflexionMemory
- `agentic-flow/src/mcp/fastmcp/tools/swarm/orchestrate.ts` — Add episode storage
- `agentic-flow/src/mcp/fastmcp/servers/stdio-full.ts` — Add memory MCP tools

**1.2 Skill Sharing via SkillLibrary**
- Agents publish successful task solutions as reusable skills
- New agents query skill library before attempting tasks
- Skill composition: combine skills for complex tasks

**Implementation**:
```typescript
const skills = new SkillLibrary(agentdb);
// After successful task
await skills.publish({ name, code, taskType, successRate, agentType });
// Before new task
const applicable = await skills.findApplicable(taskDescription, { limit: 3 });
```

**1.3 Pattern Storage via ReasoningBank**
- Store learned patterns with vector embeddings
- Replace Jaccard similarity with HNSW semantic search
- Enable cross-domain pattern transfer

**Expected Impact**:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Agent learning curve | Flat (no memory) | Exponential (replay) | 10-100x |
| Skill reuse | 0% | 60-80% | New capability |
| Pattern matching | Jaccard (O(n)) | HNSW (<100µs) | 1000x faster |
| State persistence | None (lost on restart) | Full (SQLite + RuVector) | New capability |

### Phase 2: Semantic Routing with RuVector (Week 3-4)

**Goal**: Replace hardcoded keyword routing with learned semantic routing.

**2.1 @ruvector/router for Agent Dispatch**
- Embed task descriptions and agent specializations as vectors
- Route tasks to agents based on semantic similarity, not keyword matching
- Learn routing preferences from outcomes

**Implementation**:
```typescript
import { Router } from '@ruvector/router';
const router = new Router();
// Register agent capabilities as embeddings
agents.forEach(a => router.addRoute(a.type, a.capabilityEmbedding));
// Route incoming task
const bestAgent = await router.route(taskEmbedding);
```

**Files to modify**:
- `src/routing/provider-matcher.ts` — Replace keyword synonyms with semantic matching
- `agentic-flow/src/mcp/fastmcp/tools/swarm/orchestrate.ts` — Semantic task routing

**2.2 @ruvector/sona for Trajectory Learning**
- Record agent decision trajectories with rewards
- Micro-LoRA updates for fast adaptation
- EWC++ prevents catastrophic forgetting of proven strategies

**Implementation**:
```typescript
import { SONA } from '@ruvector/sona';
const sona = new SONA();
// Record trajectory
sona.recordStep({ state: taskState, action: agentChoice, reward });
// Get learned policy
const optimalAction = await sona.predict(currentState);
```

**2.3 CausalMemoryGraph for Evidence-Based Routing**
- Track which (agent_type, task_type) pairs produce best outcomes
- Do-calculus: `do(assign_to_coder) → 0.9 success` vs `do(assign_to_researcher) → 0.3 success`
- Causal uplift guides routing decisions

**Expected Impact**:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Routing accuracy | ~50% (keyword) | 85-95% (semantic + causal) | 40-90% |
| Adaptation speed | Never (static) | Per-session (SONA) | New capability |
| Forgetting prevention | N/A | EWC++ | New capability |

### Phase 3: Attention-Based Agent Coordination (Week 5-6)

**Goal**: Use attention mechanisms for intelligent agent communication and decision-making.

**3.1 HyperbolicAttention for Hierarchical Teams**
- Model agent hierarchies in hyperbolic space (tree-like structure)
- Leader agents attend more broadly, worker agents attend to local context
- Natural fit for hierarchical swarm topologies

**3.2 FlashAttention for Large-Scale Agent Communication**
- O(n) memory instead of O(n²) for agent-to-agent attention
- Scale to 100+ agents without memory explosion
- Tiled computation for hardware efficiency

**3.3 MoE (Mixture of Experts) for Specialized Dispatch**
- Each "expert" corresponds to an agent type
- Gating network learns which expert handles which query type
- Top-k selection ensures only relevant agents are consulted

**3.4 MultiHeadAttention for Multi-Factor Decisions**
- Different heads attend to different aspects (urgency, complexity, domain)
- Combined attention score drives agent selection
- Learnable attention weights adapt to task distribution

**Files to modify**:
- `packages/agentdb/src/controllers/AttentionService.ts` — Already has 5 mechanisms
- `agentic-flow/src/mcp/fastmcp/tools/swarm/orchestrate.ts` — Add attention-weighted routing
- New: `agentic-flow/src/coordination/attention-coordinator.ts`

**Expected Impact**:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Coordination efficiency | Manual rules | Attention-weighted | +200% |
| Agent team scaling | ~10 agents | 100+ agents (FlashAttention) | 10x |
| Specialization | Static roles | Learned MoE routing | Adaptive |

### Phase 4: Graph-Based State Management (Week 7-8)

**Goal**: Replace flat storage with hypergraph-based state.

**4.1 @ruvector/graph-node for Agent State**
- Episodes, skills, causal relationships as graph nodes
- Temporal hyperedges track state evolution over time
- Cypher queries for complex relationship traversal

**Implementation**:
```typescript
import { GraphDB } from '@ruvector/graph-node';
const graph = new GraphDB();
// Store agent relationships
await graph.addEdge('agent:coder', 'completed', 'task:auth-impl', {
  duration: 120, success: true, reward: 0.95
});
// Query: "Which agents successfully completed auth tasks?"
const results = await graph.query(
  'MATCH (a:Agent)-[r:completed]->(t:Task) WHERE t.type = "auth" AND r.success = true RETURN a, r.reward'
);
```

**4.2 SyncCoordinator for Multi-Instance**
- Bidirectional sync between agentic-flow instances
- Conflict-free merge of learned patterns
- QUIC transport for sub-second synchronization

**4.3 GraphDatabaseAdapter Integration**
- Connect existing adapter to AgentDB initialization
- Store all episodes, skills, causal edges as graph nodes
- Enable relationship-aware queries

**Expected Impact**:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Query capability | Key-value lookup | Cypher graph traversal | +500% |
| Relationship tracking | None | Hypergraph with temporal edges | New capability |
| Multi-instance sync | None | Sub-second (QUIC) | New capability |

### Phase 5: Self-Improvement Pipeline (Week 9-10)

**Goal**: Automated continuous improvement without human intervention.

**5.1 NightlyLearner for Automated Discovery**
- Background job analyzes accumulated episodes
- Discovers new causal patterns
- Runs A/B experiments on routing hypotheses
- Prunes low-confidence patterns

**5.2 LearningSystem with 9 RL Algorithms**
- Q-Learning, SARSA for simple routing policies
- DQN, PPO for complex multi-step decisions
- MCTS for planning-heavy tasks
- A3C for multi-agent coordination

**5.3 ExplainableRecall for Auditable Decisions**
- Every routing decision backed by Merkle proof
- Minimal hitting set: which facts justify this assignment?
- Policy compliance certificates for enterprise use

**5.4 @ruvector/sona Background Training**
- Zero-cost background training during idle periods
- Micro-LoRA updates don't interfere with active operations
- EWC++ preserves proven strategies while learning new ones

**Expected Impact**:
| Metric | Before | After | Improvement |
|--------|--------|-------|-------------|
| Learning mode | None | 9 RL algorithms + SONA | New capability |
| Pattern discovery | Manual | Automated (NightlyLearner) | Autonomous |
| Decision explainability | Black box | Merkle proofs | Enterprise-ready |
| Continuous learning | None | EWC++ (no forgetting) | New capability |

### Phase 6: MCP Tool Exposure (Week 11-12)

**Goal**: Expose all integration capabilities as MCP tools.

New MCP tools for `stdio-full.ts`:

| Tool | Parameters | Description |
|------|-----------|-------------|
| `memory_episode_store` | episode, agentType, reward | Store agent episode |
| `memory_episode_recall` | query, limit, threshold | Recall similar episodes |
| `skill_publish` | name, code, taskType | Publish agent skill |
| `skill_find` | description, limit | Find applicable skills |
| `route_semantic` | taskDescription | Semantic agent routing |
| `route_causal` | taskType, agentTypes | Causal routing decision |
| `attention_coordinate` | agents, task, mechanism | Attention-weighted coordination |
| `graph_query` | cypher | Graph state query |
| `graph_store` | nodes, edges | Store graph state |
| `learning_trajectory` | steps, reward | Record learning trajectory |
| `learning_predict` | state | Predict optimal action |
| `explain_decision` | decisionId | Get Merkle proof explanation |

## Consequences

### Positive
- Agents learn from experience instead of starting fresh every session
- Routing accuracy improves from ~50% (keyword) to 85-95% (semantic + causal)
- Cross-agent skill sharing enables exponential learning curves
- Graph-based state enables complex relationship queries
- Self-improvement pipeline runs autonomously
- All decisions become explainable with Merkle proofs
- 70% of installed RuVector capabilities become actively used

### Negative
- Significant implementation effort (~12 weeks for full integration)
- AgentDB becomes a hard dependency for agentic-flow
- Learning overhead adds latency to first few operations (cold start)
- Graph state increases storage requirements
- Complexity increases significantly vs current simple Maps

### Risks
- SONA micro-LoRA updates may conflict with attention mechanism weights
- Hypergraph queries may be slower than simple key-value for trivial lookups
- EWC++ parameter tuning requires experimentation
- NightlyLearner A/B experiments need careful isolation

### Migration Path
- Phase 1-2 can run alongside existing Maps (gradual migration)
- Each phase is independently valuable (no all-or-nothing)
- Fallback to in-memory Maps if AgentDB unavailable (graceful degradation)

## Related ADRs

- **ADR-051**: MCP Tool Implementation Gap (tools to expose)
- **ADR-052**: CLI Tool Gap Remediation (CLI wrappers for new tools)
- **ADR-054**: AgentDB V3 Architecture Review (controller inventory)
- **ADR-056**: RVF/RuVector Integration Roadmap (package-level integration)

## Progress

Phase 6 (MCP tools) complete. Phases 1-5 integration wiring implemented.

## References

- AgentDB Controllers: `packages/agentdb/src/controllers/` (21 files)
- RuVectorBackend: `packages/agentdb/src/backends/ruvector/RuVectorBackend.ts`
- RuVectorLearning: `packages/agentdb/src/backends/ruvector/RuVectorLearning.ts`
- GraphDatabaseAdapter: `packages/agentdb/src/backends/graph/GraphDatabaseAdapter.ts`
- AttentionService: `packages/agentdb/src/controllers/AttentionService.ts`
- Agentic-Flow Integration Stub: `src/mcp/agentdb-integration.ts`
- Agentic-Flow Learning Service: `src/services/agentdb-learning.service.ts`
- MCP Server: `agentic-flow/src/mcp/fastmcp/servers/stdio-full.ts`
- Task Orchestrator: `agentic-flow/src/mcp/fastmcp/tools/swarm/orchestrate.ts`
- Routing: `src/routing/provider-matcher.ts`, `severity-classifier.ts`, `escalation-router.ts`
- @ruvector/router: `packages/agentdb/node_modules/@ruvector/router/`
- @ruvector/sona: `packages/agentdb/node_modules/@ruvector/sona/`
- @ruvector/graph-node: `packages/agentdb/node_modules/@ruvector/graph-node/`
